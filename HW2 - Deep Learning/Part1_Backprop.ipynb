{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wm-gRDthHLX3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 1: Backpropagation\n",
    "<a id=part1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ac5T7hs3HLX5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this part, we'll implement **backpropagation** and **automatic differentiation** from scratch and compare our implementations to `PyTorch`'s built in implementation (`autograd`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsFtfxIVHW0Q"
   },
   "source": [
    "### Porting to Google Colab\n",
    "The following cell enables this notebook to run from Google Colab as well as from your local machine IDE.<br>\n",
    "You can change `root_directory` and/or `this_notebook_google_path` to point to the directory in your Google account, which contains this notebook, together with the `hw2` sub-directory and its class files, the `imgs` sub-directory and the rest of the files.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "21wMFj3vHbJZ"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    import sys\n",
    "    import os\n",
    "    root_directory = '/content/gdrive/'\n",
    "    this_notebook_google_path = root_directory + 'Othercomputers/My Laptop/projects/RUNI/DL_TA/hw2'\n",
    "    drive.mount(root_directory)\n",
    "    # enable import python files from this notebook's path\n",
    "    sys.path.append(this_notebook_google_path)\n",
    "    # enable reading images and data files from this notebook's path\n",
    "    os.chdir(this_notebook_google_path)\n",
    "except:\n",
    "    # no Google Colab --> fall back to local machine\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "kGUc_X_uIS-K"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_single_image(file_name: str) -> None:\n",
    "    # Load the images\n",
    "    image1 = mpimg.imread(file_name)\n",
    "\n",
    "    # Create subplots with 1 row and 1 columns\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "    # Plot the first image on the left subplot\n",
    "    axes.imshow(image1)\n",
    "    axes.axis('off')  # Turn off axis\n",
    "    axes.set_title(file_name)  # Set title\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "l0VWWp6LHLX6",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import unittest\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "test = unittest.TestCase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkgbCYVPHLX8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reminder: The backpropagation algorithm is at the core of training deep models. To state the problem we'll tackle in this notebook, imagine we have an L-layer MLP model, defined as\n",
    "$$\n",
    "\\hat{\\vec{y}^i} = \\vec{y}_L^i= \\varphi_L \\left(\n",
    "\\mat{W}_L \\varphi_{L-1} \\left( \\cdots\n",
    "\\varphi_1 \\left( \\mat{W}_1 \\vec{x}^i + \\vec{b}_1 \\right)\n",
    "\\cdots \\right)\n",
    "+ \\vec{b}_L \\right),\n",
    "$$\n",
    "\n",
    "a pointwise loss function $\\ell(\\vec{y}, \\hat{\\vec{y}})$ and an empirical loss over our entire data set,\n",
    "$$\n",
    "L(\\vec{\\theta}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(\\vec{y}^i, \\hat{\\vec{y}^i}) + R(\\vec{\\theta})\n",
    "$$\n",
    "\n",
    "where $\\vec{\\theta}$ is a vector containing all network parameters, e.g.\n",
    "$\\vec{\\theta} = \\left[ \\mat{W}_{1,:}, \\vec{b}_1, \\dots,  \\mat{W}_{L,:}, \\vec{b}_L \\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXiI0JRMHLX9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to train our model we would like to calculate the derivative\n",
    "(or **gradient**, in the multivariate case) of the loss with respect to each and every one of the parameters,\n",
    "i.e. $\\pderiv{L}{\\mat{W}_j}$ and $\\pderiv{L}{\\vec{b}_j}$ for all $j$.\n",
    "Since the gradient \"points\" to the direction of functional increase, the negative gradient is often used as a descent direction for descent-based optimization algorithms.\n",
    "In other words, iteratively updating each parameter proportianally to it's negetive gradient can lead to\n",
    "convergence to a local minimum of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Y2fYvciHLX9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Calculus tells us that as long as we know the derivatives of all the functions \"along the way\"\n",
    "($\\varphi_i(\\cdot),\\ \\ell(\\cdot,\\cdot),\\ R(\\cdot)$)\n",
    "we can use the **chain rule** to calculate the derivative\n",
    "of the loss with respect to any one of the parameter vectors.\n",
    "Note that if the loss $L(\\vec{\\theta})$ is scalar (which is usually the case), the gradient of a parameter\n",
    "will have the same shape as the parameter itself (matrix/vector/tensor of same dimensions).\n",
    "\n",
    "For deep models that are a composition of many functions, calculating the gradient of each parameter by hand and implementing hard-coded gradient derivations quickly becomes infeasible.\n",
    "Additionally, such code makes models hard to change, since any change potentially requires re-derivation and re-implementation of the entire gradient function.\n",
    "\n",
    "The backpropagation algorithm, which we saw [in the lecture](https://vistalab-technion.github.io/cs236781/lecture_notes/lecture_03/#error-backpropagation), provides us with a effective method of applying the **chain rule** recursively so that we can implement gradient calculations of arbitrarily deep or complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuXh9LchHLX9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll now implement backpropagation using a modular approach, which will allow us to chain many components layers together and get automatic gradient calculation of the output with respect to the input or any intermediate parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hew4EsusHLX-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To do this, we'll define a `Layer` class. Here's the API of this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "6gFoxrd4HLX-",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Layer in module hw2.layers:\n",
      "\n",
      "class Layer(abc.ABC)\n",
      " |  A Layer is some computation element in a network architecture which\n",
      " |  supports automatic differentiation using forward and backward functions.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Layer\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  backward(self, dout)\n",
      " |      Computes the backward pass of the layer, i.e. the gradient\n",
      " |      calculation of the final network output with respect to each of the\n",
      " |      parameters of the forward function.\n",
      " |      :param dout: The gradient of the network with respect to the\n",
      " |      output of this layer.\n",
      " |      :return: A tuple with the same number of elements as the parameters of\n",
      " |      the forward function. Each element will be the gradient of the\n",
      " |      network output with respect to that parameter.\n",
      " |  \n",
      " |  forward(self, *args, **kwargs)\n",
      " |      Computes the forward pass of the layer.\n",
      " |      :param args: The computation arguments (implementation specific).\n",
      " |      :return: The result of the computation.\n",
      " |  \n",
      " |  params(self)\n",
      " |      :return: Layer's trainable parameters and their gradients as a list\n",
      " |      of tuples, each tuple containing a tensor and it's corresponding\n",
      " |      gradient tensor.\n",
      " |  \n",
      " |  train(self, training_mode=True)\n",
      " |      Changes the mode of this layer between training and evaluation (test)\n",
      " |      mode. Some layers have different behaviour depending on mode.\n",
      " |      :param training_mode: True: set the model in training mode. False: set\n",
      " |      evaluation mode.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset({'backward', 'forward', 'params'})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hw2.layers as layers\n",
    "help(layers.Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBAoGO8XHLX_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In other words, a `Layer` can be anything: a layer, an activation function, a loss function or generally *any computation that we know how to derive a gradient for*.\n",
    "\n",
    "Each `Layer` must define a `forward()` function and a `backward()` function.\n",
    "- The `forward()` function performs the actual calculation/operation of the block and returns an output.\n",
    "- The `backward()` function computes the gradient of the **input and parameters** as a function of the gradient of the **output**, according to the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKEoOyJSHLX_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here's a diagram illustrating the above explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "DurnYsHCIveH"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_single_image(\"imgs/backprop.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlTKnVtbHLX_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that the diagram doesn't show that if the function is parametrized, i.e. $f(\\vec{x},\\vec{y})=f(\\vec{x},\\vec{y};\\vec{w})$, there are also gradients to calculate for the parameters $\\vec{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZMNWqlIHLYA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The forward pass is straightforward: just do the computation.\n",
    "To understand the backward pass, imagine that there's some \"downstream\" loss function\n",
    "$L(\\vec{\\theta})$ and magically somehow we are told the gradient of that loss with respect\n",
    "to the **output** $\\vec{z}$ of our block, i.e. $\\pderiv{L}{\\vec{z}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhBId6ohHLYA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, since we know how to calculate the derivative of $f(\\vec{x},\\vec{y};\\vec{w})$,\n",
    "it means we know how to calculate $\\pderiv{\\vec{z}}{\\vec{x}}$, $\\pderiv{\\vec{z}}{\\vec{y}}$ and $\\pderiv{\\vec{z}}{\\vec{w}}$ .\n",
    "Thanks to the chain rule, this is all we need to calculate the gradients of the **loss** w.r.t. the input and\n",
    "parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pderiv{L}{\\vec{x}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{x}}\\\\\n",
    "\\pderiv{L}{\\vec{y}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{y}}\\\\\n",
    "\\pderiv{L}{\\vec{w}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZtHlW2PHLYA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Comparison with PyTorch\n",
    "<a id=part1_1></a>\n",
    "\n",
    "PyTorch has the [`nn.Module`](https://pytorch.org/docs/stable/nn.html#module) base class, which may seem to be similar to our `Layer` since it also represents a computation element in a network.\n",
    "However PyTorch's `nn.Module`s don't compute the gradient directly, they only define the forward calculations.\n",
    "Instead, PyTorch has a more low-level API for defining a function and explicitly implementing it's `forward()` and `backward()`. See [`autograd.Function`](https://pytorch.org/docs/stable/autograd.html#function).\n",
    "When an operation is performed on a tensor, it creates a `Function` instance which performs the operation and\n",
    "stores any necessary information for calculating the gradient later on. Additionally, `Functions`s point to the\n",
    "other `Function` objects representing the operations performed earlier on the tensor. Thus, a graph (or DAG)\n",
    "of operations is created (this is not 100% exact, as the graph is actually composed of a different type of class which wraps the backward method, but it's accurate enough for our purposes).\n",
    "\n",
    "A `Tensor` instance which was created by performing operations on one or more tensors with `requires_grad=True`, has a `grad_fn` property which is a `Function` instance representing the last operation performed to produce this tensor.\n",
    "This exposes the graph of `Function` instances, each with it's own `backward()` function. Therefore, in PyTorch the `backward()` function is called on the tensors, not the modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOkmp-iIHLYA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our `Layer`s are therefore a combination of the ideas in `Module` and `Function` and we'll implement them together,\n",
    "just to make things simpler.\n",
    "Our goal here is to create a \"poor man's autograd\": We'll use PyTorch tensors,\n",
    "but we'll calculate and store the gradients in our `Layer`s (or return them).\n",
    "The gradients we'll calculate are of the entire block, not individual operations on tensors.\n",
    "\n",
    "To test our implementation, we'll use PyTorch's `autograd`.\n",
    "\n",
    "Note that of course this method of tracking gradients is **much** more limited than what PyTorch offers. However it allows us to implement the backpropagation algorithm very simply and really see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEEARYjfHLYA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's set up some testing instrumentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "eBSavC8IHLYA",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hw2.grad_compare import compare_layer_to_torch\n",
    "\n",
    "def test_block_grad(block: layers.Layer, x, y=None, delta=1e-3):\n",
    "    diffs = compare_layer_to_torch(block, x, y)\n",
    "\n",
    "    # Assert diff values\n",
    "    for diff in diffs:\n",
    "        test.assertLess(diff, delta)\n",
    "\n",
    "# Show the compare function\n",
    "compare_layer_to_torch??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J1WI_gyHLYB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notes:\n",
    "- After you complete your implementation, you should make sure to read and understand the `compare_layer_to_torch()` function. It will help you understand what PyTorch is doing.\n",
    "- The value of `delta` above is should not be needed. A correct implementation will give you a `diff` of exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6ofxm-HLYB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Layer Implementations\n",
    "<a id=part1_2></a>\n",
    "\n",
    "We'll now implement some `Layer`s that will enable us to later build an MLP model of arbitrary depth, complete with automatic differentiation.\n",
    "\n",
    "For each block, you'll first implement the `forward()` function.\n",
    "Then, you will calculate the derivative of the block by hand with respect to each of its\n",
    "input tensors and each of its parameter tensors (if any).\n",
    "Using your manually-calculated derivation, you can then implement the `backward()` function.\n",
    "\n",
    "Notice that we have intermediate Jacobians that are potentially high dimensional tensors.\n",
    "For example in the expression\n",
    "$\\pderiv{L}{\\vec{w}} = \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}$,\n",
    "the term $\\pderiv{\\vec{z}}{\\vec{w}}$ is a 4D Jacobian if both $\\vec{z}$ and $\\vec{w}$\n",
    "are 2D matrices.\n",
    "\n",
    "In order to implement the backpropagation algorithm efficiently,\n",
    "we need to implement every backward function without explicitly constructing this\n",
    "Jacobian. Instead, we're interested in directly calculating the vector-Jacobian product\n",
    "(VJP) $\\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}$.\n",
    "In order to do this, you should try to figure out the gradient of the loss with respect to\n",
    "one element, e.g. $\\pderiv{L}{\\vec{w}_{1,1}}$ and extrapolate from there how to\n",
    "directly obtain the VJP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRefQpf6HLYB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xeFP3knHLYB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Leaky) ReLU\n",
    "\n",
    "ReLU, or rectified linear unit is a very common activation function in deep learning architectures.\n",
    "In it's most standard form, as we'll implement here, it has no parameters.\n",
    "\n",
    "We'll first implement the \"leaky\" version, defined as\n",
    "\n",
    "$$\n",
    "\\mathrm{relu}(\\vec{x}) = \\max(\\alpha\\vec{x},\\vec{x}), \\ 0\\leq\\alpha<1\n",
    "$$\n",
    "\n",
    "This is similar to the ReLU activation we've seen in class, only that it has a small non-zero slope then it's input is negative.\n",
    "Note that it's not strictly differentiable, however it has sub-gradients, defined separately any positive-valued input and for negative-valued input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAl3kxj4HLYB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `LeakyReLU` class in the `hw2/layers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "9vNse4CPHLYB",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "in_features = 200\n",
    "num_classes = 10\n",
    "eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "NiJh3wQmHLYB",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing gradients... \n",
      "input    diff=0.000\n"
     ]
    }
   ],
   "source": [
    "# Test LeakyReLU\n",
    "alpha = 0.1\n",
    "lrelu = layers.LeakyReLU(alpha=alpha)\n",
    "x_test = torch.randn(N, in_features)\n",
    "\n",
    "# Test forward pass\n",
    "z = lrelu(x_test)\n",
    "test.assertSequenceEqual(z.shape, x_test.shape)\n",
    "test.assertTrue(torch.allclose(z, torch.nn.LeakyReLU(alpha)(x_test), atol=eps))\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(lrelu, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBOm8m6nHLYB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now using the LeakyReLU, we can trivially define a regular ReLU block as a special case.\n",
    "\n",
    "**TODO**: Complete the implementation of the `ReLU` class in the `hw2/layers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "OJZseAmbHLYB",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing gradients... \n",
      "input    diff=0.000\n"
     ]
    }
   ],
   "source": [
    "# Test ReLU\n",
    "relu = layers.ReLU()\n",
    "x_test = torch.randn(N, in_features)\n",
    "\n",
    "# Test forward pass\n",
    "z = relu(x_test)\n",
    "test.assertSequenceEqual(z.shape, x_test.shape)\n",
    "test.assertTrue(torch.allclose(z, torch.relu(x_test), atol=eps))\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(relu, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6K4597uHLYC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Sigmoid\n",
    "\n",
    "The sigmoid function $\\sigma(x)$ is also sometimes used as an activation function.\n",
    "We have also seen it previously in the context of logistic regression.\n",
    "\n",
    "The sigmoid function is defined as\n",
    "\n",
    "$$\n",
    "\\sigma(\\vec{x}) = \\frac{1}{1+\\exp(-\\vec{x})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "ypqL8WgoHLYC",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing gradients... \n",
      "input    diff=0.000\n"
     ]
    }
   ],
   "source": [
    "# Test Sigmoid\n",
    "sigmoid = layers.Sigmoid()\n",
    "x_test = torch.randn(N, in_features, in_features) # 3D input should work\n",
    "\n",
    "# Test forward pass\n",
    "z = sigmoid(x_test)\n",
    "test.assertSequenceEqual(z.shape, x_test.shape)\n",
    "test.assertTrue(torch.allclose(z, torch.sigmoid(x_test), atol=eps))\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(sigmoid, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymRfAuP3HLYC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Hyperbolic Tangent\n",
    "\n",
    "The hyperbolic tangent function $\\tanh(x)$ is a common activation function used when the output should be in the range \\[-1, 1\\].\n",
    "\n",
    "The tanh function is defined as\n",
    "\n",
    "$$\n",
    "\\tanh(\\vec{x}) = \\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-\\vec{x})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "QGqa43yZHLYC",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing gradients... \n",
      "input    diff=0.000\n"
     ]
    }
   ],
   "source": [
    "# Test TanH\n",
    "tanh = layers.TanH()\n",
    "x_test = torch.randn(N, in_features, in_features) # 3D input should work\n",
    "\n",
    "# Test forward pass\n",
    "z = tanh(x_test)\n",
    "test.assertSequenceEqual(z.shape, x_test.shape)\n",
    "test.assertTrue(torch.allclose(z, torch.tanh(x_test), atol=eps))\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(tanh, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nc8ftU-jHLYC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Linear (fully connected) layer\n",
    "\n",
    "First, we'll implement an affine transform layer, also known as a fully connected layer.\n",
    "\n",
    "Given an input $\\mat{X}$ the layer computes,\n",
    "\n",
    "$$\n",
    "\\mat{Z} = \\mat{X} \\mattr{W}  + \\vec{b} ,~\n",
    "\\mat{X}\\in\\set{R}^{N\\times D_{\\mathrm{in}}},~\n",
    "\\mat{W}\\in\\set{R}^{D_{\\mathrm{out}}\\times D_{\\mathrm{in}}},~ \\vec{b}\\in\\set{R}^{D_{\\mathrm{out}}}.\n",
    "$$\n",
    "\n",
    "Notes:\n",
    "- We write it this way to follow the implementation conventions.\n",
    "- $N$ is the number of samples in the input (batch size). The input $\\mat{X}$ will always be a tensor containing a batch dimension first.\n",
    "- Thanks to broadcasting, $\\vec{b}$ can remain a vector even though the input $\\mat{X}$ is a matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Tfqk5w-HLYC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `Linear` class in the `hw2/layers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "tbJN5HXAHLYD",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing gradients... \n",
      "input    diff=0.000\n",
      "param#01 diff=0.000\n",
      "param#02 diff=0.000\n",
      "Comparing gradients... \n",
      "input    diff=0.000\n",
      "param#01 diff=0.000\n",
      "param#02 diff=0.000\n"
     ]
    }
   ],
   "source": [
    "# Test Linear\n",
    "out_features = 1000\n",
    "fc = layers.Linear(in_features, out_features)\n",
    "x_test = torch.randn(N, in_features)\n",
    "\n",
    "# Test forward pass\n",
    "z = fc(x_test)\n",
    "test.assertSequenceEqual(z.shape, [N, out_features])\n",
    "torch_fc = torch.nn.Linear(in_features, out_features,bias=True)\n",
    "torch_fc.weight = torch.nn.Parameter(fc.w)\n",
    "torch_fc.bias = torch.nn.Parameter(fc.b)\n",
    "test.assertTrue(torch.allclose(torch_fc(x_test), z, atol=eps))\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(fc, x_test)\n",
    "\n",
    "# Test second backward pass\n",
    "x_test = torch.randn(N, in_features)\n",
    "z = fc(x_test)\n",
    "z = fc(x_test)\n",
    "test_block_grad(fc, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6MVUPboHLYD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cross-Entropy Loss\n",
    "\n",
    "As you know by know, cross-entropy is a common loss function for classification tasks.\n",
    "In class, we defined it as\n",
    "\n",
    "$$\\ell_{\\mathrm{CE}}(\\vec{y},\\hat{\\vec{y}}) = - {\\vectr{y}} \\log(\\hat{\\vec{y}})$$\n",
    "\n",
    "where $\\hat{\\vec{y}} = \\mathrm{softmax}(x)$ is a probability vector (the output of softmax on the class scores $\\vec{x}$) and the vector $\\vec{y}$ is a 1-hot encoded class label.\n",
    "\n",
    "However, it's tricky to compute the gradient of softmax, so instead we'll define a version of cross-entropy that produces the exact same output but works directly on the class scores $\\vec{x}$.\n",
    "\n",
    "We can write,\n",
    "$$\\begin{align}\n",
    "\\ell_{\\mathrm{CE}}(\\vec{y},\\hat{\\vec{y}}) &= - {\\vectr{y}} \\log(\\hat{\\vec{y}})\n",
    "= - {\\vectr{y}} \\log\\left(\\mathrm{softmax}(\\vec{x})\\right) \\\\\n",
    "&= - {\\vectr{y}} \\log\\left(\\frac{e^{\\vec{x}}}{\\sum_k e^{x_k}}\\right) \\\\\n",
    "&= - \\log\\left(\\frac{e^{x_y}}{\\sum_k e^{x_k}}\\right) \\\\\n",
    "&= - \\left(\\log\\left(e^{x_y}\\right) - \\log\\left(\\sum_k e^{x_k}\\right)\\right)\\\\\n",
    "&= - x_y + \\log\\left(\\sum_k e^{x_k}\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "Where the scalar $y$ is the correct class label, so $x_y$ is the correct class score.\n",
    "\n",
    "Note that this version of cross entropy is also what's [provided](https://pytorch.org/docs/stable/nn.html#crossentropyloss) by PyTorch's `nn` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8opkHjRDHLYD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `CrossEntropyLoss` class in the `hw2/layers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "JjpH_jCIHLYD",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 2.7283618450164795\n",
      "Comparing gradients... \n",
      "input    diff=0.000\n"
     ]
    }
   ],
   "source": [
    "# Test CrossEntropy\n",
    "cross_entropy = layers.CrossEntropyLoss()\n",
    "scores = torch.randn(N, num_classes)\n",
    "labels = torch.randint(low=0, high=num_classes, size=(N,), dtype=torch.long)\n",
    "\n",
    "# Test forward pass\n",
    "loss = cross_entropy(scores, labels)\n",
    "expected_loss = torch.nn.functional.cross_entropy(scores, labels)\n",
    "test.assertLess(torch.abs(expected_loss-loss).item(), 1e-5)\n",
    "print('loss=', loss.item())\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(cross_entropy, scores, y=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdwB_8WLHLYD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building Models\n",
    "<a id=part1_3></a>\n",
    "\n",
    "Now that we have some working `Layer`s, we can build an MLP model of arbitrary depth and compute end-to-end gradients.\n",
    "\n",
    "First, lets copy an idea from PyTorch and implement our own version of the `nn.Sequential` `Module`.\n",
    "This is a `Layer` which contains other `Layer`s and calls them in sequence. We'll use this to build our MLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_dDPS6gHLYD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `Sequential` class in the `hw2/layers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "l_2Co3zCHLYE",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing gradients... \n",
      "input    diff=0.000\n",
      "param#01 diff=0.000\n",
      "param#02 diff=0.000\n",
      "param#03 diff=0.000\n",
      "param#04 diff=0.000\n",
      "param#05 diff=0.000\n",
      "param#06 diff=0.000\n",
      "param#07 diff=0.000\n",
      "param#08 diff=0.000\n",
      "param#09 diff=0.000\n",
      "param#10 diff=0.000\n",
      "param#11 diff=0.000\n",
      "param#12 diff=0.000\n",
      "param#13 diff=0.000\n",
      "param#14 diff=0.000\n"
     ]
    }
   ],
   "source": [
    "# Test Sequential\n",
    "# Let's create a long sequence of layers and see\n",
    "# whether we can compute end-to-end gradients of the whole thing.\n",
    "\n",
    "seq = layers.Sequential(\n",
    "    layers.Linear(in_features, 100),\n",
    "    layers.Linear(100, 200),\n",
    "    layers.Linear(200, 100),\n",
    "    layers.ReLU(),\n",
    "    layers.Linear(100, 500),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.Linear(500, 200),\n",
    "    layers.ReLU(),\n",
    "    layers.Linear(200, 500),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Linear(500, 1),\n",
    "    layers.Sigmoid(),\n",
    ")\n",
    "x_test = torch.randn(N, in_features)\n",
    "\n",
    "# Test forward pass\n",
    "z = seq(x_test)\n",
    "test.assertSequenceEqual(z.shape, [N, 1])\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(seq, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQFc_qvIHLYE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, equipped with a `Sequential`, all we have to do is create an MLP architecture.\n",
    "We'll define our MLP with the following hyperparameters:\n",
    "- Number of input features, $D$.\n",
    "- Number of output classes, $C$.\n",
    "- Sizes of hidden layers, $h_1,\\dots,h_L$.\n",
    "\n",
    "So the architecture will be:\n",
    "\n",
    "FC($D$, $h_1$) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "FC($h_1$, $h_2$) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "$\\cdots$ $\\rightarrow$\n",
    "FC($h_{L-1}$, $h_L$) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "FC($h_{L}$, $C$)\n",
    "\n",
    "We'll also create a sequence of the above MLP and a cross-entropy loss, since it's the gradient of the loss that we need in order to train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnN3Eyz9HLYE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `MLP` class in the `hw2/layers.py` module. Ignore the `dropout` parameter for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "k5LZVwDtHLYE",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP, Sequential\n",
      "\t[0] Linear(self.in_features=200, self.out_features=100)\n",
      "\t[1] ReLU\n",
      "\t[2] Linear(self.in_features=100, self.out_features=50)\n",
      "\t[3] ReLU\n",
      "\t[4] Linear(self.in_features=50, self.out_features=100)\n",
      "\t[5] ReLU\n",
      "\t[6] Linear(self.in_features=100, self.out_features=10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an MLP model\n",
    "mlp = layers.MLP(in_features, num_classes, hidden_features=[100, 50, 100])\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "g6rIqVXbHLYF",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP loss=2.30924391746521, activation=relu\n",
      "Comparing gradients... \n",
      "input    diff=0.000\n",
      "param#01 diff=0.000\n",
      "param#02 diff=0.000\n",
      "param#03 diff=0.000\n",
      "param#04 diff=0.000\n",
      "param#05 diff=0.000\n",
      "param#06 diff=0.000\n",
      "param#07 diff=0.000\n",
      "param#08 diff=0.000\n",
      "MLP loss=2.3934404850006104, activation=sigmoid\n",
      "Comparing gradients... \n",
      "input    diff=0.000\n",
      "param#01 diff=0.000\n",
      "param#02 diff=0.000\n",
      "param#03 diff=0.000\n",
      "param#04 diff=0.000\n",
      "param#05 diff=0.000\n",
      "param#06 diff=0.000\n",
      "param#07 diff=0.000\n",
      "param#08 diff=0.000\n"
     ]
    }
   ],
   "source": [
    "# Test MLP architecture\n",
    "N = 100\n",
    "in_features = 10\n",
    "num_classes = 10\n",
    "for activation in ('relu', 'sigmoid'):\n",
    "    mlp = layers.MLP(in_features, num_classes, hidden_features=[100, 50, 100], activation=activation)\n",
    "    test.assertEqual(len(mlp.sequence), 7)\n",
    "\n",
    "    num_linear = 0\n",
    "    for b1, b2 in zip(mlp.sequence, mlp.sequence[1:]):\n",
    "        if (str(b2).lower() == activation):\n",
    "            test.assertTrue(str(b1).startswith('Linear'))\n",
    "            num_linear += 1\n",
    "\n",
    "    test.assertTrue(str(mlp.sequence[-1]).startswith('Linear'))\n",
    "    test.assertEqual(num_linear, 3)\n",
    "\n",
    "    # Test MLP gradients\n",
    "    # Test forward pass\n",
    "    x_test = torch.randn(N, in_features)\n",
    "    labels = torch.randint(low=0, high=num_classes, size=(N,), dtype=torch.long)\n",
    "    z = mlp(x_test)\n",
    "    test.assertSequenceEqual(z.shape, [N, num_classes])\n",
    "\n",
    "    # Create a sequence of MLPs and CE loss\n",
    "    seq_mlp = layers.Sequential(mlp, layers.CrossEntropyLoss())\n",
    "    loss = seq_mlp(x_test, y=labels)\n",
    "    test.assertEqual(loss.dim(), 0)\n",
    "    print(f'MLP loss={loss}, activation={activation}')\n",
    "\n",
    "    # Test backward pass\n",
    "    test_block_grad(seq_mlp, x_test, y=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELA_IV0LHLYF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If the above tests passed then congratulations - you've now implemented an arbitrarily deep model and loss function with end-to-end automatic differentiation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWc_R4YjHLYF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Questions\n",
    "<a id=part2_7></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE0qYRbqHLYF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw2/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "W1BS86Y1HLYF",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw2.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPj_VDmVHLYF",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Suppose we have a linear (i.e. fully-connected) layer with a weight tensor $\\mat{W}$, defined with `in_features=1024` and `out_features=512`. We apply this layer to an input tensor $\\mat{X}$ containing a batch of `N=64` samples. The output of the layer is denoted as $\\mat{Y}$.\n",
    "\n",
    "1. Consider the Jacobian tensor $\\pderiv{\\mat{Y}}{\\mat{X}}$ of the output of the layer w.r.t. the input $\\mat{X}$.\n",
    "    1. What is the shape of this tensor?\n",
    "    1. Is this Jacobian sparse (most elements zero by definition)? If so, why and which elements?\n",
    "    1. Given the gradient of the output w.r.t. some downstream scalar loss $L$, $\\delta\\mat{Y}=\\pderiv{L}{\\mat{Y}}$, do we need to materialize the above Jacobian in order to calculate the downstream gratdient w.r.t. to the input ($\\delta\\mat{X}$)? If yes, explain why; if no, show how to calcualte it without materializing the Jacobian.\n",
    "\n",
    "1. Consider the Jacobian tensor $\\pderiv{\\mat{Y}}{\\mat{W}}$ of the output of the layer w.r.t. the layer weights $\\mat{W}$. Answer questions A-C about it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "2VxuK8CBHLYF",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "1.\n",
       "\n",
       "\n",
       "A) The Jacobian tensor of the output will have the shape (N, out_features, in_features), in our case is (64, 514, 1024).<br />\n",
       "B) W is considered sparse if it is initialized using a normal distribution with a low standard deviation.<br />\n",
       "C)  Since this is a linear transformation, the Jacobian of the transformation with respect to X is W^T, and it doesn't need to be explicitly calculated.<br />\n",
       "For the derivative with respect to W:<br />\n",
       "\n",
       "2.\n",
       "\n",
       "\n",
       "A) The shape of the partial derivative of the transformation with respect to W matches the shape of X, which is 64x512.<br />\n",
       "B) X can be sparse depending on the input, but it isn't necessarily required for the model to learn efficiently, unlike W. <br />\n",
       "C) The derivative with respect to W is simply X, as seen in the implementation of the linear layer, so there's no need to explicitly calculate the Jacobian.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw2.answers.part1_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7pOtmpPHLYF",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Is back-propagation **required** in order to train neural networks with decent-based optimization? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "8mNXljDxHLYF",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Backpropagation is not the only method for training neural networks. It is the most commonly used method because, in most classification tasks, the output is a scalar, allowing gradients to be explicitly and efficiently calculated. This makes it straightforward to use the chain rule to compute the infinitesimal change in the loss function with respect to each parameter in the network.\n",
       "\n",
       "In all models, there must be some objective function that is minimized, though not necessarily through backpropagation. For example, you can have an MLP where the last layer is trained using Least Squares optimization. The previous layers can be initialized randomly, and learning can be achieved by pruning unnecessary connections within the second-to-last layer.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw2.answers.part1_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
